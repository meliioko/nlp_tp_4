{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780cf295-7141-43e8-b8b8-199b14071e2a",
   "metadata": {},
   "source": [
    "# Coding your own RNN\n",
    "\n",
    "Using this pre-filled notebook, we will code our own RNN for sentence classification. For now, we'll keep using IMDB, as the goal of this part is to understand how an RNN works.\n",
    "\n",
    "Unlike our previous lab, we will also learn the embedding layer. Which means we need to deal with vocabulary by ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f26c411e-1cd5-4a1c-9b3d-de16e26db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec040734-ff61-4c85-982c-acb2a5bd6d8c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We load the dataset and split the training set in a stratified train/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "42b598f9-36b5-43e3-bb25-488b0fb53aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/aeschylli/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 266.07it/s]\n",
      "Loading cached split indices for dataset at /home/aeschylli/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-5f37fd0866e4f89f.arrow and /home/aeschylli/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-dd5732a0e6ac784c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309e444-d52d-4f87-8d18-7bb470b7cfcd",
   "metadata": {},
   "source": [
    "## Vocabulary (1 point)\n",
    "\n",
    "**\\[1 point\\]** Build your own vocabulary. The [example provided in torchtext documentation](https://pytorch.org/text/stable/vocab.html#id1) might be of help.\n",
    "* Don't forge to setup the `min_freq` parameter to not include unfrequent noise.\n",
    "* You will need a tokenizer. Reuse the `basic_english` one from the our previous lab.\n",
    "* For an RNN we need two special tokens: `<unk>`, for unknown words, and `<pad>` for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a19f11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "words = [word for sentence in train_df['text'] for word in tokenizer(sentence)]\n",
    "counter = Counter(words)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "unk_token = '<unk>'\n",
    "pad_token = '<pad>'\n",
    "default_index = -1\n",
    "vocabulary = vocab(ordered_dict=ordered_dict, specials=[unk_token, pad_token], min_freq=3)\n",
    "vocabulary.set_default_index(default_index)\n",
    "vocabulary.set_default_index(vocabulary[unk_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "777fb3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89386d66-f758-4d87-b786-9d570eb22f2f",
   "metadata": {},
   "source": [
    "## Vectorize and batch the input (3 points)\n",
    "\n",
    "As seen in class, our model should take one-hot encoded vectors corresponding to the each token vocabulary id. However, computing a vector x matrix multiplication for every input is unnecessarily costly. Multiplying a one-hot vector with a matrix is the equivalent of taking one row of the matrix. In pyTorch, we provide ids for each token which will be used as input to an `nn.Embedding` layer. The id is simply the row in the embedding matrix.\n",
    "\n",
    "**\\[1 point\\]** Fill the `vectorize_text` function returning a 1D torch tensor of `torch.long` for each input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f6fc4eb-2f25-43ee-8b2f-a9154545ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    text: str, vocabulary: Vocab, tokenizer: Callable[[str], List[str]]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a tensor of vocabluary IDs for a given text.\n",
    "    Args:\n",
    "        text: the input text.\n",
    "        vocabulary: a Vocab objects.\n",
    "        tokenizer: a text tokenizer.\n",
    "    Returns:\n",
    "        A tensor of IDs (torch.long).\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text)\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        res.append(vocabulary[token])\n",
    "    return torch.tensor(res, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a4a9058-ad04-43b9-bf5c-680afb44e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = partial(vectorize_text, vocabulary=vocabulary, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717208d-2f50-4d16-b170-904beeeb71ad",
   "metadata": {},
   "source": [
    "Check the function is working correctly, especially it should return the right special id for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9727204a-ecff-4e4d-a316-543865d52a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  56, 3160,   13,  244,  526,   50,    3,    3,    3,    0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"Some text I am thinking about... ragafqfa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97eb948c-cfae-47b1-b8ce-bafa358afe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:15<00:00, 1272.09it/s]\n",
      "100%|██████████| 5000/5000 [00:04<00:00, 1191.77it/s]\n",
      "100%|██████████| 25000/25000 [00:18<00:00, 1348.85it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = [text_pipeline(text) for text in tqdm(train_df[\"text\"])]\n",
    "y_train = train_df[\"label\"]\n",
    "X_valid = [text_pipeline(text) for text in tqdm(valid_df[\"text\"])]\n",
    "y_valid = valid_df[\"label\"]\n",
    "X_test = [text_pipeline(text) for text in tqdm(test_df[\"text\"])]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bee7a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  158,   184,  1400,   189,   275,   168,   232,   168,   189,   254,\n",
       "          170,   156,   601,   157,   851,   150,   155,   150,   154,   172,\n",
       "          152,   566,   167,   210,  1085,   229,   150,   157,   150,   155,\n",
       "          152,  1802,   167,   156,   270,   326,  2561,   306,   153,   428,\n",
       "          201,   206,   157,   152,   312,   228,   179,   194,  1104,   150,\n",
       "          203,   157,   150,   161,   215,   183,   250,   152,   173,   156,\n",
       "          270,   283,   255,   153,   153,   158,   290,   164,   167,   533,\n",
       "          160,   158,   242,   164,   173,   156,   150,   154,   201,   176,\n",
       "          335,   158,   224,   299,   165,   160,   637, 12184,   154,   176,\n",
       "         3554,  1001,   155,   596,   156,   530,   157,   759,   157,   286,\n",
       "          276,   305,   159,   150,   159,   154,   165,   160, 10747,   491,\n",
       "        16244,   533,   154,   249,   316,   207,   154,   175,   606,  1199,\n",
       "         4425, 20690,   182,   150,   307,   153,   257,  1433,   216,   154,\n",
       "          163,   642,   164,   173,   155,   249,  3554,   184,   443,   156,\n",
       "          324,   201,   176,   183,   179,   268,   161,   400,   154,   199,\n",
       "          248,   159,   177,   680,   386,   519,   152,   573,   157,   159,\n",
       "         8404,  1633,   154,   152,   576,   338,   159,   198,   159,   150,\n",
       "          159,   153])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].size()[0]\n",
    "X_train[1].add_(150)\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67138763-f848-4dd7-9036-68df352d22a0",
   "metadata": {},
   "source": [
    "To speed up the training process, we turn the inputs into batches, as we did last time. For batches to work, every line must have the same lengths. Last time, it was implicit as only a vector (the average of all embeddings) was provided. This time, every line has the length of a different review.\n",
    "\n",
    "To go around this problem, we use padding. So every line within a batch is padded to the length of its longest element.\n",
    "\n",
    "* **\\[1 point\\]** Fill the data generator function.\n",
    "* **\\[1 point\\]** On which side should you pad and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a88dd162-8107-4f05-bc8a-2fd313a3b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    X: List[torch.tensor], y: List[int], pad_id: int, batch_size: int = 32\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Yield batches from given input data and labels.\n",
    "    Args:\n",
    "        X: a list of tensor (input features).\n",
    "        y: the corresponding labels.\n",
    "        batch_size: the size of every batch [32].\n",
    "    Returns:\n",
    "        A tuple of tensors (features, labels).\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        max_length = 0\n",
    "        for j in range(i, i + batch_size):\n",
    "            if X[j].size()[0] > max_length:\n",
    "                max_length = X[j].size()[0]\n",
    "        for j in range(i, i + batch_size):\n",
    "            pad = max_length - X[j].size()[0]\n",
    "            pad_tensor = torch.full((pad,), pad_id)\n",
    "            X[j] = torch.cat((X[j], pad_tensor))\n",
    "            \n",
    "        yield (torch.stack(X[i:i+batch_size]), torch.tensor(y[i:i+batch_size], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c2e10342-4d6b-45c4-8e05-68ef6375b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train, vocabulary[pad_token])\n",
    "valid_gen = lambda: data_generator(X_valid, y_valid, vocabulary[pad_token])\n",
    "test_gen = lambda: data_generator(X_test, y_test, vocabulary[pad_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489d625-f1ad-49ea-8b54-74463e04877f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classifier (3 points)\n",
    "\n",
    "**\\[3 points\\]** Code your own RNN. Fill the `RNN` class correctly. Remember, an RNN has 3 elements.\n",
    "* An embedding layer to turn one-hot vectors into dense vectors.\n",
    "* A linear layer which takes the concatenation of the current embedding and hidden layer and send it to the hidden layer of our next step.\n",
    "  * `embedding_size + hidden_size -> hidden_size`\n",
    "* A linear layer which takes the concatenation of the current embedding and hidden layer and send it to the output.\n",
    "  * `embedding_size + hidden_size -> 1`\n",
    "* The provided `init_hidden` function sho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cfeb7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.hidden = nn.Linear(embedding_size, hidden_size)\n",
    "        self.previous = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Concatenate the embedded input and hidden state\n",
    "        current = self.hidden(embedded)\n",
    "\n",
    "        # Pass the concatenated tensor through the linear layer\n",
    "        previous = self.previous(hidden)\n",
    "\n",
    "        # Pass the concatenated tensor through the output layer\n",
    "        output = self.output(torch.tanh(current + previous))\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c665df6-ff9a-4b66-80fe-8abe62d895c9",
   "metadata": {},
   "source": [
    "## Training (2 points)\n",
    "\n",
    "Training is a bit different than usual. We will need to sequentially (but in \"batch parallel\") go through an input, keeping track of the hidden layer, and use the last output as prediction.\n",
    "\n",
    "**\\[2 point\\]** Code the training loop.\n",
    "* Note that for each batch, you need to loop through the whole input and use the output of the last token as input to your criterion.\n",
    "* Keep the best model evaluated on the validation set.\n",
    "* Plot the training and validation losses.\n",
    "* Training will take some time (~30 min on a T4 GPU). Make sure your results appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "965f407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, num_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        t_batch = 0\n",
    "\n",
    "        for inputs, targets in train_loader():\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden = model.init_hidden(inputs.size(1))\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "\n",
    "            # Use the output of the last token as the prediction\n",
    "            last_output = outputs[:,-1,:]\n",
    "            targets = targets.unsqueeze(-1).float()\n",
    "            loss = criterion(last_output, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            t_batch +=1\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        train_loss /= t_batch\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        v_batch = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader():\n",
    "                hidden = model.init_hidden(inputs.size(1))\n",
    "                outputs, _ = model(inputs, hidden)\n",
    "\n",
    "                last_output = outputs[:,-1]\n",
    "                print(last_output)\n",
    "                targets = targets.view(-1,1).float()\n",
    "                loss = criterion(last_output, targets)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                v_batch += 1\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        valid_loss /= v_batch\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Check if current model has the best validation loss\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0e9053b8-19f3-45b4-a434-2f8cef8a1f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "46cdc558-249b-4c26-b1e5-fc1c0a78d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embedding = 32\n",
    "n_hidden = 64\n",
    "model = RNN(len(vocabulary.get_itos()), n_embedding, n_hidden).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dc500-ece2-4efa-8207-758a5f428c26",
   "metadata": {},
   "source": [
    "## Evaluation (1 point)\n",
    "\n",
    "* **\\[1 point\\]** Compute the accuracy for all 3 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a4544dfd-7567-4bea-b484-490b6a7b74e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[242], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained \u001b[39m=\u001b[39m train(model, criterion,optimizer, train_gen, valid_gen, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[239], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(last_output, targets)\n\u001b[1;32m     25\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     28\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m t_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/scia/nlp/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/scia/nlp/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/scia/nlp/venv/lib/python3.10/site-packages/torch/optim/rmsprop.py:118\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    114\u001b[0m     momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, grads, square_avgs, momentum_buffer_list, grad_avgs)\n\u001b[0;32m--> 118\u001b[0m     rmsprop(\n\u001b[1;32m    119\u001b[0m         params_with_grad,\n\u001b[1;32m    120\u001b[0m         grads,\n\u001b[1;32m    121\u001b[0m         square_avgs,\n\u001b[1;32m    122\u001b[0m         grad_avgs,\n\u001b[1;32m    123\u001b[0m         momentum_buffer_list,\n\u001b[1;32m    124\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    125\u001b[0m         alpha\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    126\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    127\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    128\u001b[0m         momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    129\u001b[0m         centered\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcentered\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    130\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    131\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    132\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    133\u001b[0m     )\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/scia/nlp/venv/lib/python3.10/site-packages/torch/optim/rmsprop.py:233\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, foreach, maximize, differentiable, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_rmsprop\n\u001b[0;32m--> 233\u001b[0m func(\n\u001b[1;32m    234\u001b[0m     params,\n\u001b[1;32m    235\u001b[0m     grads,\n\u001b[1;32m    236\u001b[0m     square_avgs,\n\u001b[1;32m    237\u001b[0m     grad_avgs,\n\u001b[1;32m    238\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    239\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    240\u001b[0m     alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    241\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    242\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    243\u001b[0m     momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    244\u001b[0m     centered\u001b[39m=\u001b[39;49mcentered,\n\u001b[1;32m    245\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    246\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/scia/nlp/venv/lib/python3.10/site-packages/torch/optim/rmsprop.py:304\u001b[0m, in \u001b[0;36m_single_tensor_rmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable)\u001b[0m\n\u001b[1;32m    302\u001b[0m     param\u001b[39m.\u001b[39madd_(buf, alpha\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mlr)\n\u001b[1;32m    303\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     param\u001b[39m.\u001b[39;49maddcdiv_(grad, avg, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained = train(model, criterion,optimizer, train_gen, valid_gen, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adb09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
